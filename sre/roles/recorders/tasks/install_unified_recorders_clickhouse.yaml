---
- name: Import tools role for variable setting tasks - ClickHouse endpoint
  ansible.builtin.import_role:
    name: tools
    tasks_from: set_clickhouse_endpoint
  vars:
    tools_cluster:
      kubeconfig: "{{ recorders_cluster.kubeconfig }}"

- name: Import tools role for variable setting tasks - ClickHouse username and password
  ansible.builtin.import_role:
    name: tools
    tasks_from: set_clickhouse_credentials
  vars:
    tools_cluster:
      kubeconfig: "{{ recorders_cluster.kubeconfig }}"

- name: Load job information
  ansible.builtin.set_fact:
    recorders_unified_clickhouse_job: "{{ lookup('ansible.builtin.file', 'files/unified/clickhouse/job.yaml') | from_yaml }}"

- name: Create PersistentVolumeClaim to retain records
  kubernetes.core.k8s:
    kubeconfig: "{{ recorders_cluster.kubeconfig }}"
    namespace: "{{ recorders_namespace.name }}"
    src: files/unified/clickhouse/persistentvolumeclaim.yaml
    state: present

- name: Create ClickHouse-based unified recorder environment list
  ansible.builtin.set_fact:
    recorders_clickhouse_env_vars:
      - name: CLICKHOUSE_ENDPOINT
        value: "{{ tools_clickhouse_endpoint }}"
      - name: CLICKHOUSE_USERNAME
        value: "{{ tools_clickhouse_username }}"
      - name: CLICKHOUSE_PASSWORD
        value: "{{ tools_clickhouse_password }}"

- name: Create ConfigMap with Python script
  kubernetes.core.k8s:
    kubeconfig: "{{ recorders_cluster.kubeconfig }}"
    namespace: "{{ recorders_namespace.name }}"
    template: templates/unified/clickhouse/configmap.j2
    state: present
  vars:
    python_script_file_contents: "{{ lookup('ansible.builtin.file', 'files/unified/clickhouse/scripts/gather.py') }}"
    requirements_file_contents: "{{ lookup('ansible.builtin.file', 'files/unified/clickhouse/scripts/requirements.txt') }}"

- name: Wait for any ongoing jobs to be removed
  kubernetes.core.k8s_info:
    api_version: "{{ recorders_unified_clickhouse_job.apiVersion }}"
    kind: "{{ recorders_unified_clickhouse_job.kind }}"
    kubeconfig: "{{ recorders_cluster.kubeconfig }}"
    name: "{{ recorders_unified_clickhouse_job.metadata.name }}"
    namespace: "{{ recorders_namespace.name }}"
  register: recorders_unified_clickhouse_job_info
  until:
    - recorders_unified_clickhouse_job_info.resources | length == 0
  retries: 8
  delay: 15

- name: Install ClickHouse unified recorder
  kubernetes.core.k8s:
    kubeconfig: "{{ recorders_cluster.kubeconfig }}"
    namespace: "{{ recorders_namespace.name }}"
    template: templates/unified/clickhouse/job.j2
    state: present
  vars:
    container_image: "{{ recorders_unified_clickhouse_job.spec.template.spec.containers[0].image }}"
    container_environment_variables: "{{ recorders_clickhouse_env_vars }}"

- name: Wait for any ongoing jobs to be removed
  kubernetes.core.k8s_info:
    api_version: batch/v1
    kind: Job
    kubeconfig: "{{ recorders_cluster.kubeconfig }}"
    label_selectors:
      - app.kubernetes.io/name=clickhouse-unified-recorder
      - app.kubernetes.io/part-of=it-bench
    namespace: "{{ recorders_namespace.name }}"
  register: recorders_kubernetes_job_info
  until:
    - recorders_kubernetes_job_info.resources | length == 0
  retries: 20
  delay: 30

- name: Create Deployment to retrieve records
  kubernetes.core.k8s:
    kubeconfig: "{{ recorders_cluster.kubeconfig }}"
    namespace: "{{ recorders_namespace.name }}"
    src: files/unified/clickhouse/deployment.yaml
    state: present
    wait: true

- name: Retrieve the retriever pod name (wait until Running/Ready)
  kubernetes.core.k8s_info:
    api_version: v1
    kind: Pod
    kubeconfig: "{{ recorders_cluster.kubeconfig }}"
    namespace: "{{ recorders_namespace.name }}"
    label_selectors:
      - app.kubernetes.io/name=clickhouse-unified-recorder
      - app.kubernetes.io/part-of=it-bench
  register: recorders_kubernetes_pods_info
  retries: 20
  delay: 6
  until:
    - recorders_kubernetes_pods_info.resources | length >= 1
    - (recorders_kubernetes_pods_info.resources[0].status.phase | default('')) == 'Running'
    - (recorders_kubernetes_pods_info.resources[0].status.containerStatuses | default([])) | length > 0
    - recorders_kubernetes_pods_info.resources[0].status.containerStatuses[0].ready | default(false)

- name: Copy records directory from pod
  kubernetes.core.k8s_cp:
    kubeconfig: "{{ recorders_cluster.kubeconfig }}"
    local_path: /tmp/topology
    namespace: "{{ recorders_kubernetes_pods_info.resources[0].metadata.namespace }}"
    pod: "{{ recorders_kubernetes_pods_info.resources[0].metadata.name }}"
    remote_path: /opt/app-root/src/records
    state: from_pod
  when:
    - recorders_kubernetes_pods_info.resources | length == 1

- name: Uninstall the Deployment
  kubernetes.core.k8s:
    kubeconfig: "{{ recorders_cluster.kubeconfig }}"
    namespace: "{{ recorders_namespace.name }}"
    src: files/unified/clickhouse/deployment.yaml
    state: absent
    wait: true

- name: Delete ConfigMap with Python script
  kubernetes.core.k8s:
    api_version: v1
    delete_all: true
    kind: ConfigMap
    kubeconfig: "{{ recorders_cluster.kubeconfig }}"
    namespace: "{{ recorders_namespace.name }}"
    label_selectors:
      - app.kubernetes.io/name=clickhouse-unified-recorder
      - app.kubernetes.io/part-of=it-bench
    state: absent
    wait: true

- name: Delete PersistentVolumeClaim
  kubernetes.core.k8s:
    kubeconfig: "{{ recorders_cluster.kubeconfig }}"
    namespace: "{{ recorders_namespace.name }}"
    src: files/unified/clickhouse/persistentvolumeclaim.yaml
    state: absent
    wait: true

- name: Find all exported JSON files
  ansible.builtin.find:
    path: /tmp/topology
    patterns:
      - "*.json"
  register: recorders_kubernetes_files

- name: Ensure observability_information directory exists on local
  ansible.builtin.file:
    path: "{{ recorders_storage.local.directory }}/observability_information"
    state: directory
    mode: "0755"
  when:
    - recorders_storage.local is defined

- name: Copy exported data into local directory
  ansible.builtin.copy:
    dest: "{{ recorders_storage.local.directory }}/observability_information/{{ file.path | basename }}"
    mode: "0644"
    src: "{{ file.path }}"
  loop: "{{ recorders_kubernetes_files.files | default([]) }}"
  loop_control:
    label: file/{{ file.path | basename }}
    loop_var: file
  when:
    - recorders_storage.local is defined
    - (recorders_kubernetes_files.matched | default(0)) > 0

- name: Upload exported data to S3 bucket
  amazon.aws.s3_object:
    endpoint_url: "{{ recorders_storage.s3.endpoint }}"
    bucket: "{{ recorders_storage.s3.bucket }}"
    object: "{{ recorders_storage.s3.directory }}/observability_information/{{ file.path | basename }}"
    src: "{{ file.path }}"
    mode: put
  loop: "{{ recorders_kubernetes_files.files | default([]) }}"
  loop_control:
    label: file/{{ file.path | basename }}
    loop_var: file
  when:
    - recorders_storage.s3 is defined
    - (recorders_kubernetes_files.matched | default(0)) > 0
